# -*- coding: utf-8 -*-
"""creditcardfraud (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ROovdSRVMf-C61G6TRCVV0H1_mJ9vSRk
"""

# Commented out IPython magic to ensure Python compatibility.
#importing packages
# %matplotlib inline
import scipy.stats as stats
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('ggplot')

"""# Exploratory Data Analysis and Data Preprocessing

### 1. Loading the dataset
The dataset is taken from <a href="https://www.kaggle.com/mlg-ulb/creditcarcredit_card_dataraud/download">Kaggle Credit Card Fraud Detection Dataset</a>
"""

from google.colab import drive
drive.mount('/content/drive')

credit_card_data = pd.read_csv('/content/drive/MyDrive/creditcard.csv')

!gdown https://drive.google.com/uc?1oyUTXe4_xPMIzHrk4BRgNLtw2F7ibTG2&export=download

"""### 2. Viewing the first 5 entries of the dataset"""

credit_card_data.head()

"""### 3. Getting the dataset shape"""

print('This data frame has {} rows and {} columns.'.format(credit_card_data.shape[0], credit_card_data.shape[1]))

"""### 4. A peek at the dataset"""

credit_card_data.sample(5)
# credit_card_data.sample returns a random sample of items from an axis of the object

"""### 5. Finding more details of the dataset """

#info
credit_card_data.info()

# checking if there are any null values in our dataset
credit_card_data.isnull().values.any()

# numerical summary -> only non-anonymized columns of interest (not the columns encoded by PCA)
pd.set_option('precision', 3) # to get the result upto 3 decimal places
credit_card_data.loc[:, ['Time', 'Amount']].describe() # Statistics of the columns not encoded by PCA

# visualizations of the time feature
plt.figure(figsize=(10,8))
plt.title('Distribution of Time Feature')
sns.distplot(credit_card_data.Time)

# visualisation of the amount feature
plt.figure(figsize=(10,8))
plt.title('Distribution of Monetary Value Feature')
sns.distplot(credit_card_data.Amount)

"""We find that - around 88 dollars is the mean of all credit card transactions in this data set. The biggest transaction had a monetary value of around 25,691 dollars."""

# fraud vs. legit transactions 
counts = credit_card_data.Class.value_counts()
legit = counts[0]
fraudulent = counts[1]
perc_legit = (legit/(legit+fraudulent))*100
perc_fraudulent = (fraudulent/(legit+fraudulent))*100
print('There were {} non-fraudulent transactions ({:.3f}%) and {} fraudulent transactions ({:.3f}%).'.format(legit, perc_legit, fraudulent, perc_fraudulent))

"""### 6. Visualising the number of fraudulent and non - fraudulent transactions in the dataset"""

plt.figure(figsize=(8,6))
sns.barplot(x = counts.index, y = counts)
plt.title('Count of Fraudulent vs. Non-Fraudulent Transactions')
plt.ylabel('Count')
plt.xlabel('Class (0:Non-Fraudulent, 1:Fraudulent)')

"""Thus, we find that our dataset is **highly unbalanced**. <br>
The number of non - fraudulent transactions is much high compared to the fraudulent transactions.

### 7. Separating the fraudulent and non - fraudulent parts of the dataset
"""

# 'Class' = 1 - fraudulent transactions
fraud = credit_card_data[credit_card_data['Class'] == 1]
# 'Class' = 0 - non - fraudulent or legit transactions
legit = credit_card_data[credit_card_data['Class'] == 0]

print(fraud.shape,legit.shape)

"""### 8. How different are the amount of money used in different transaction classes?"""

# statistical measure of the data
fraud.Amount.describe()

legit.Amount.describe()

# compare the values for both transactions
credit_card_data.groupby('Class').mean()

f, (ax1, ax2) = plt.subplots(2, 1, sharex = True)
f.suptitle('Amount per transaction by class')
bins = 50
ax1.hist(fraud.Amount, bins = bins)
ax1.set_title('Fraudulent')
ax2.hist(legit.Amount, bins = bins)
ax2.set_title('Non - Fraudulent')
plt.xlabel('Amount')
plt.ylabel('Number of Transactions')
plt.xlim((0, 20000))
plt.yscale('log')
plt.show()

"""### 9. Checking if fraudulent transactions occur more often during certain time frame"""

f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)
f.suptitle('Time of transaction vs Amount by class')
ax1.scatter(fraud.Time, fraud.Amount)
ax1.set_title('Fraud')
ax2.scatter(legit.Time, legit.Amount)
ax2.set_title('legit')
plt.xlabel('Time (in Seconds)')
plt.ylabel('Amount')
plt.show()

"""### 10. Finding correlations - that is determining how different features affect the Class (Fraud or not)"""

corr = credit_card_data.corr()
corr

# heatmap - uses color in order to communicate a value to the reader.
corr = credit_card_data.corr()
plt.figure(figsize = (12,10))
heat = sns.heatmap(data = corr)
plt.title('Heatmap of Correlation')

"""From the heatmap, we get an idea of to what degree different features contribute to the transaction being fraudulent or not."""

# skewness
# Finding the skewness of the features 
# to ensure that they are not much deviated from the Gaussian distribution
# As presence of much skewness in features may violate our training algo assumptions
skew_ = credit_card_data.skew()
skew_

"""# Data Preprocessing

### 1. Scaling Amount and Time
"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler2 = StandardScaler()
# scaling the time column
scaled_time = scaler.fit_transform(credit_card_data[['Time']])
flat_list1 = [item for sublist in scaled_time.tolist() for item in sublist]
scaled_time = pd.Series(flat_list1)

# scaling the amount column
scaled_amount = scaler2.fit_transform(credit_card_data[['Amount']])
flat_list2 = [item for sublist in scaled_amount.tolist() for item in sublist]
scaled_amount = pd.Series(flat_list2)

# concatenating newly created scaled columns with original credit_card_data
credit_card_data = pd.concat([credit_card_data, scaled_amount.rename('scaled_amount'), scaled_time.rename('scaled_time')], axis=1)
# viewing a random sample of items from an axis of the object
credit_card_data.sample(5)

# dropping old (unscaled) amount and time columns
credit_card_data.drop(['Amount', 'Time'], axis = 1, inplace = True)

"""### 2. Splitting Data into Train and Test sets"""

# manual train test split using numpy's random.rand
mask = np.random.rand(len(credit_card_data)) < 0.9
print(mask.shape)
train = credit_card_data[mask]
test = credit_card_data[~mask]
print('Train Shape: {}\nTest Shape: {}'.format(train.shape, test.shape))

# resetting the indices 
train.reset_index(drop = True, inplace = True)
test.reset_index(drop = True, inplace = True)

"""### 3. Creating a subsample data set with balanced class distributions"""

# how many random samples from legit transactions do we need?
no_of_frauds = train.Class.value_counts()[1]
print('There are {} fraudulent transactions in the train data.'.format(no_of_frauds))

# storing the non - fraudulent and fraudulent transactions in the train data
non_fraud = train[train['Class'] == 0]
fraud = train[train['Class'] == 1]

selected = non_fraud.sample(no_of_frauds)
selected.shape

# printing the first 5 selected items
selected.head()

# resetting the indices
selected.reset_index(drop = True, inplace = True)
fraud.reset_index(drop = True, inplace = True)

# into a subsample data set with equal class distribution
subsample = pd.concat([selected, fraud])
len(subsample)

# shuffling our data set
subsample = subsample.sample(frac = 1).reset_index(drop = True)

subsample.head(10)

"""### 4. Visualisation of fraud and non - fraud classes in subsample dataset created"""

new_counts = subsample.Class.value_counts()
plt.figure(figsize=(8,6))
sns.barplot(x=new_counts.index, y=new_counts)
plt.title('Count of Fraudulent vs. Non-Fraudulent Transactions In Subsample')
plt.ylabel('Count')
plt.xlabel('Class (0:Non-Fraudulent, 1:Fraudulent)')

"""We find that, the distribution of fraud and non - fraud transactions is balanced in our subsample dataset unlike the original highly unbalanced dataset"""

# taking a look at correlations once more
corr = subsample.corr()
corr = corr[['Class']]
corr

# negative correlations smaller than -0.5
corr[corr.Class < -0.5]

# positive correlations greater than 0.5
corr[corr.Class > 0.5]

# visualizing the features with high negative correlation
f, axes = plt.subplots(nrows=2, ncols=4, figsize=(26,16))

f.suptitle('Features With High Negative Correlation', size=35)
sns.boxplot(x="Class", y="V3", data=subsample, ax=axes[0,0])
sns.boxplot(x="Class", y="V9", data=subsample, ax=axes[0,1])
sns.boxplot(x="Class", y="V10", data=subsample, ax=axes[0,2])
sns.boxplot(x="Class", y="V12", data=subsample, ax=axes[0,3])
sns.boxplot(x="Class", y="V14", data=subsample, ax=axes[1,0])
sns.boxplot(x="Class", y="V16", data=subsample, ax=axes[1,1])
sns.boxplot(x="Class", y="V17", data=subsample, ax=axes[1,2])
f.delaxes(axes[1,3])

# visualizing the features with high positive correlation
f, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,9))

f.suptitle('Features With High Positive Correlation', size=20)
sns.boxplot(x="Class", y="V4", data=subsample, ax=axes[0])
sns.boxplot(x="Class", y="V11", data=subsample, ax=axes[1])

"""### 5. Extreme Outlier Removal"""

# Only removing the extreme outliers
Q1 = subsample.quantile(0.25)
Q3 = subsample.quantile(0.75)
IQR = Q3 - Q1

credit_card_data2 = subsample[~((subsample < (Q1 - 2.5 * IQR)) |(subsample > (Q3 + 2.5 * IQR))).any(axis=1)]
print(credit_card_data2)

len_after = len(credit_card_data2)
len_before = len(subsample)
len_difference = len(subsample) - len(credit_card_data2)
print('We reduced our data size from {} transactions by {} transactions to {} transactions.'.format(len_before, len_difference, len_after))

"""### 6. Dimensionality Reduction"""

from sklearn.manifold import TSNE

X = credit_card_data2.drop('Class', axis=1)
y = credit_card_data2['Class']

# t-SNE
X_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)

# t-SNE scatter plot
import matplotlib.patches as mpatches

f, ax = plt.subplots(figsize=(24,16))


blue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')
red_patch = mpatches.Patch(color='#AF0000', label='Fraud')

ax.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)
ax.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)
ax.set_title('t-SNE', fontsize=14)

ax.grid(True)

ax.legend(handles=[blue_patch, red_patch])

"""# Classification Algorithms"""

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn

# train test split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.head()

conda install -c anaconda py-xgboost

!pip install xgboost

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier

"""## Spot - Checking Algorithms 

Spot-checking algorithms is about getting a quick assessment of a bunch of different algorithms on your machine learning problem so that you know what algorithms to focus on and what to discard.
"""

# Spot - Checking Algorithms

models = []

models.append(('LR', LogisticRegression()))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('SVM', SVC()))
models.append(('XGB', XGBClassifier()))
models.append(('RF', RandomForestClassifier()))

# testing models

results = []
names = []

for name, model in models:
    kfold = KFold(n_splits=10, random_state=42,shuffle=True)
    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='roc_auc')
    results.append(cv_results)
    names.append(name)
    msg = '%s: %f (%f)' % (name, cv_results.mean(), cv_results.std())
    print(msg)

# Compare Algorithms

fig = plt.figure(figsize=(12,10))
plt.title('Comparison of Classification Algorithms')
plt.xlabel('Algorithm')
plt.ylabel('ROC-AUC Score')
plt.boxplot(results)
ax = fig.add_subplot(111)
ax.set_xticklabels(names)
plt.show()

"""We find that RF, XGBOOST Algorithms give the greatest test accuracy and hence proceed with this algorithm to train our model"""

conda install -c anaconda graphviz

# visualizing RF
RandomForest_model = RandomForestClassifier(n_estimators = 10)

# Train
RandomForest_model.fit(X_train, y_train)

# Extract single tree
estimator = RandomForest_model.estimators_[5]

from sklearn.tree import export_graphviz
# Export as dot file
export_graphviz(estimator, out_file='tree.dot', 
                feature_names = X.columns.tolist(),
                class_names = ['0',' 1'],
                rounded = True, proportion = False, 
                precision = 2, filled = True)

# Convert to png using system command (requires Graphviz)
from subprocess import call
call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])

# Display in jupyter notebook
from IPython.display import Image
Image(filename = 'tree.png')

# testing the model 
RandomForest_predict = RandomForest_model.predict(X_test)

# printing the test set results
print(RandomForest_predict)

# Checking different accuracies for the RF model

from sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score,recall_score

rf_accuracy = accuracy_score(y_test, RandomForest_predict)
rf_recall = recall_score(y_test, RandomForest_predict)
rf_cm = confusion_matrix(y_test, RandomForest_predict)
rf_auc = roc_auc_score(y_test, RandomForest_predict)

print("Model has a Score Accuracy: {:.3%}".format(rf_accuracy))
print("Model has a Score Recall: {:.3%}".format(rf_recall))
print("Model has a Score ROC AUC: {:.3%}".format(rf_auc))

import seaborn as sns
sns.heatmap(rf_cm, annot = True)

"""Here, 0 indicates non - fraudulent transactions and 1 indicates fraudulent transactions. As seen from the confusion matrix, our RF model correctly predicted **80** transactions as non - fraudulent ( **True Negatives** ) and **38** transactions as fraudulent ( **True Positives** ). Whereas, **6** transactions which were fraudulent are predicted to be non - fraudulent ( **False Negatives** ) and **1** non - fraudulent transaction is predicted to be fraudulent ( **False positives** ). This can be visualised better as follows -"""

group_names = ['True Neg','False Pos','False Neg','True Pos']
group_counts = ["{0:0.0f}".format(value) for value in
                rf_cm.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in
                     rf_cm.flatten()/np.sum(rf_cm)]
labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(rf_cm, annot=labels, fmt='', cmap='Blues')

from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(RandomForest_model , X_test, y_test)
plt.show()

"""Thus, random forest model performs fairly well for the given highly unbalanced dataset. But, we have 6 false negatives and a false positive here. That is, the fraudulent transactions are detected to be safe or legit ones, which is not a good outcome. Therefore, let us see if we can improve our results further by using the XGBoost Algorithm."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
xgb = XGBClassifier()
xgb.fit(X_train, y_train)

xgb_predict = xgb.predict(X_test)

xgb_accuracy = accuracy_score(y_test, xgb_predict)
xgb_recall = recall_score(y_test, xgb_predict)
xgb_cm = confusion_matrix(y_test, xgb_predict)
xgb_auc = roc_auc_score(y_test, xgb_predict)

print("Model has a Score Accuracy: {:.3%}".format(xgb_accuracy))
print("Model has a Score Recall: {:.3%}".format(xgb_recall))
print("Model has a Score ROC AUC: {:.3%}".format(xgb_auc))

import seaborn as sns
sns.heatmap(xgb_cm, annot = True)

group_names = ['True Neg','False Pos','False Neg','True Pos']
group_counts = ["{0:0.0f}".format(value) for value in
                xgb_cm.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in
                     xgb_cm.flatten()/np.sum(xgb_cm)]
labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(xgb_cm, annot=labels, fmt='', cmap='Blues')

from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(xgb , X_test, y_test)
plt.show()

"""Thus, we find that XGBoost model outperforms RF model by a very slight margin. Thus, it would be appropriate to use any of the two for our task."""

